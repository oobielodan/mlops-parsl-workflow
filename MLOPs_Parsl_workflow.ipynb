{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a21bdaa-9df8-4503-bfc0-39e2d7efa766",
   "metadata": {},
   "source": [
    "# MLOPs Parsl workflow\n",
    "\n",
    "This notebook is the stand-alone companion to the Parsl MLOPs workflow in `main.py` in this repository. This notebook is designed to be run directly on an HPC resource while the `main.py` in this workflow uses the `parsl_utils` to launch MLOPs applications from a central coordinating node (i.e. a laptop or the Parallel Works platform). This workflow simulates a typical MLOPs situation with the following tasks:\n",
    "1. start an MLFlow tracking server\n",
    "2. start DVC tracking within an architve repository + remote\n",
    "3. download and preprocess training data\n",
    "4. run training loop and store results on-the-fly with MLFlow\n",
    "5. commit and push resulting models with DVC to repo + remote\n",
    "6. use the model for inference and generate figures.\n",
    "7. reusing the model for inference and generating figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768cea9-9406-4d21-8d60-2d78ff2b9182",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34799ce1-6bf0-4862-a4ae-f0f089701bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda installs\n",
    "! conda install -y -c conda-forge tensorflow\n",
    "! conda install -y -c conda-forge matplotlib\n",
    "! conda install -y -c conda-forge pandas\n",
    "! conda install -y -c conda-forge dvc \n",
    "\n",
    "# pip installs\n",
    "! pip install mlflow\n",
    "! pip install 'parsl[monitoring, visualization]' # Conda does not install monitoring, so use pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b018e6-3147-4123-ab59-40146b498675",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Based on the instructions in the [Parsl Tutorial](https://parsl.readthedocs.io/en/latest/1-parsl-introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576f915-2745-4aef-843e-4aa1986d6130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# ml dependencies\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "# mlflow dependencies\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# parsl dependencies\n",
    "import parsl\n",
    "import logging\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import Config\n",
    "from parsl.executors import HighThroughputExecutor # we want to use monitoring, so we must use HTEX\n",
    "from parsl.monitoring.monitoring import MonitoringHub\n",
    "from parsl.addresses import address_by_hostname\n",
    "\n",
    "#=================================================\n",
    "# Log everything to stdout (ends up in pink boxes \n",
    "# in the notebook). This information is logged anyway\n",
    "# in ./runinfo/<run_id>/parsl.log\n",
    "#parsl.set_stream_logger() # <-- log everything to stdout\n",
    "#==================================================\n",
    "\n",
    "print(parsl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6be29-0198-4394-9c9c-94b2f3e10635",
   "metadata": {},
   "source": [
    "# Configure Parsl\n",
    "\n",
    "This configuration must use the HTEX since we also want to enable [Parsl monitoring](https://parsl.readthedocs.io/en/latest/userguide/monitoring.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57095f-292c-4b32-ae49-8382ec2fdede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "   executors=[\n",
    "       HighThroughputExecutor(\n",
    "           label=\"local_htex\",\n",
    "           cores_per_worker=1,\n",
    "           max_workers_per_node=2,\n",
    "           address=address_by_hostname(),\n",
    "       )\n",
    "   ],\n",
    "   monitoring=MonitoringHub(\n",
    "       hub_address=address_by_hostname(),\n",
    "       hub_port=55055,\n",
    "       monitoring_debug=False,\n",
    "       resource_monitoring_interval=10,\n",
    "   ),\n",
    "   strategy='none'\n",
    ")\n",
    "\n",
    "# Loading the configuration starts a Parsl DataFlowKernel\n",
    "dfk = parsl.load(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c0f08-efee-4c9e-8191-f6373e7238e4",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 1 - direct shell invocation to background\n",
    "\n",
    "This step can be done at any point provided that a database file exists.  The default location of this file is in `./runinfo/monitoring.db` and this file is created when the Parsl configuration is loaded. When the notebook kernel is restarted, additional Parsl workflow runs' information is appended to the monitoring information in `./runinfo`. It is possible to view this information \"offline\" (i.e. no active running Parsl workflows, see Option 3, at the end of this notebook).\n",
    "\n",
    "This launch is commented out here since it is also possible to launch `parsl-visualize` from a Parsl app within the workflow, which is done below. This command is retained as a functional example. The advantage to running `parsl-visualize` as a Parsl app is that the visualization server is up and running while the workflow is running and then is shut down when the workflow is cleaned up. Otherwise, when `parsl-visualize` is launched via `os.system` the running child process can persist even after workflow shut down or notebook kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444b703-daee-4ffa-b111-7c7dcd277c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch Parsl \n",
    "#os.system('parsl-visualize 1> parsl_vis.stdout 2> parsl_vis.stderr &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471719b-de46-473f-8444-295ad8e51914",
   "metadata": {},
   "source": [
    "## Define Parsl apps\n",
    "\n",
    "Parsl workflows are divided into the smallest unit of execution, the app. There are two types of Parsl apps:\n",
    "1. Python apps are useful when launching pure Python code (i.e. TensorFlow)\n",
    "2. Bash apps are useful when launching tasks on the command line (i.e. starting the MLFlow server)\n",
    "\n",
    "Here, the applications are *defined* but not run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddeb06-9d92-4290-bb7f-048b1f506ddb",
   "metadata": {},
   "source": [
    "### Python Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c288da-dbd2-49a4-aaad-8a67c472d171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model directory for saving outputs\n",
    "env_name = \"digits_env\" # <name of your env>\n",
    "model_dir = './model-dir' \n",
    "\n",
    "@python_app\n",
    "def make_dir(model_dir):\n",
    "    import os\n",
    "    model_dir = './model-dir'\n",
    "    return os.makedirs(model_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a816c-b89a-440e-9e8e-7fd9acae916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def build_model(inputs=[], outputs=[]):\n",
    "\n",
    "    # imports ---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "    # ml dependencies\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "\n",
    "    # mlflow dependencies\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # sampling --------------------------------------------------------------------------------------------\n",
    "    \n",
    "    @keras.saving.register_keras_serializable()\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = ops.shape(z_mean)[0]\n",
    "            dim = ops.shape(z_mean)[1]\n",
    "            epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "            return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "   \n",
    "    # build encoder ---------------------------------------------------------------------------------------\n",
    "    \n",
    "    latent_dim = 2\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    print(encoder.summary())\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # build decoder ---------------------------------------------------------------------------------------\n",
    "    \n",
    "    latent_dim = 2\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    print(decoder.summary())\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # model -----------------------------------------------------------------------------------------------\n",
    "    \n",
    "    @keras.saving.register_keras_serializable()\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = ops.mean(\n",
    "                    ops.sum(\n",
    "                        keras.losses.binary_crossentropy(data, reconstruction),\n",
    "                        axis=(1, 2),\n",
    "                    )\n",
    "                )\n",
    "                kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "                kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # build model -----------------------------------------------------------------------------------------    \n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa911e8-c432-4f08-baaf-062c9cad73f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def train_model(inputs=[], outputs=[]): # inputs = [num, model, data, experiment]\n",
    "    model_dir = './model-dir' \n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\n",
    "    \n",
    "    # if the model has already been trained at least once, load that model\n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): \n",
    "        inputs[1].load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "    \n",
    "    mlflow.autolog() # start autologging\n",
    "    \n",
    "    run_name = f\"{inputs[0]}_test\" # define a run name for this iteration of training\n",
    "    artifact_path = f\"{inputs[0]}\"  # define an artifact path that the model will be saved to\n",
    "    \n",
    "    # initiate the MLflow run context \n",
    "    # - training needs to happen inside of the mlflow run or you will run into problems with double logging\n",
    "    with mlflow.start_run(run_name = run_name, experiment_id = inputs[3]) as run:\n",
    "        \n",
    "        history = inputs[1].fit(inputs[2], epochs=30, batch_size=128, callbacks = [early_stopping_cb])\n",
    "        inputs[1].save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "\n",
    "        hist_pd = pd.DataFrame(history.history)\n",
    "        hist_pd.to_csv(os.path.join(model_dir, f'history_{inputs[0]}.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc570-c425-4eda-9c57-73191da7d946",
   "metadata": {},
   "source": [
    "### Bash Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109f82b-c83f-4fcc-8718-6254ed1b5ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def start_parsl_visualize(stdout='parsl_vis_app.stdout', stderr='parsl_vis_app.stderr'):\n",
    "    return 'parsl-visualize --listen 127.0.0.1 --port 8080'\n",
    "\n",
    "@bash_app\n",
    "def start_mlflow(stdout='mlflow_app.stdout', stderr='mlflow_app.stderr'):\n",
    "    return 'mlflow server --host 127.0.0.1 --port 8081'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df7db9-922a-4871-af78-2c475dfdc854",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 2 - Monitoring as a Parsl app\n",
    "\n",
    "This approach is helpful if we want Parsl Monitoring processes to be cleaned up after the workflow is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea30929-87b0-442d-a5aa-ee9cd7a7ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Parsl visualization in a\n",
    "# separate cell since we only want\n",
    "# to run this app one time. This\n",
    "# invocation of parsl_visualize is\n",
    "# technically part of the workflow.\n",
    "\n",
    "parsl_future = start_parsl_visualize()\n",
    "mlflow_future = start_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f20000-a71e-4702-b224-590d719504fd",
   "metadata": {},
   "source": [
    "## Run the workflow\n",
    "\n",
    "The workflow code below runs the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eea978-6be7-4611-b7ff-a6afc11500b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future = make_dir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc0bc3-54bd-4c5c-95bd-792ce46ecace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7138b23-a5f5-4d15-93fc-b98ea3c991be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439ff77-9b88-44b3-801b-07e668d472b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utilize and set up the initialized server for tracking \n",
    "client = MlflowClient(tracking_uri = \"http://127.0.0.1:8081\")\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8081\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869afd68-78ed-4c27-9008-bcc33aa174d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# provide an experiment description that will appear in the UI\n",
    "experiment1_description = (\n",
    "    \"This is the digits forecasting project.\"\n",
    "    \"This experiment contains the digit model for randomized numbers (0-9) trained separately.\"\n",
    ")\n",
    "\n",
    "# provide searchable tags for the experiment\n",
    "experiment1_tags = {\n",
    "    \"project_name\": \"digit-forecasting\",\n",
    "    \"model_type\": \"randomzied\",\n",
    "    \"team\": \"digit-ml\",\n",
    "    \"project_quarter\": \"Q3-2024\",\n",
    "    \"mlflow.note.content\": experiment1_description,\n",
    "}\n",
    "\n",
    "# create the experiment and give it a unique name\n",
    "digit_experiment1 = client.create_experiment(\n",
    "    name=\"Randomize_Model\", tags=experiment1_tags\n",
    ")\n",
    "\n",
    "digit_experiment1 = mlflow.set_experiment(\"Randomize_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24366c45-8279-4d62-80a3-d6d448e91b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c63c0f-dfa8-4e28-bc3a-0a07f238df02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990b4c7-7aa4-4066-a1c1-883ad28e7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94da8b-8b03-483a-8eb3-4f3d48b18edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retraining the model n times\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.expand_dims(np.concatenate([x_train, x_test], axis=0), -1).astype(\"float32\") / 255\n",
    "\n",
    "for arr in np.array_split(mnist_digits, n):\n",
    "    count += 1\n",
    "    train_model([f\"rand_{count}\", model, arr, digit_experiment1.experiment_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742f3ea-2d2d-407f-948b-bdb583c15184",
   "metadata": {},
   "source": [
    "## Stop Parsl\n",
    "\n",
    "The cells above can be rerun any number of times; this will simply send more and more apps to be run by Parsl. When the workflow is truly complete, it is time to call the cleanup() command. This command runs implicitly when a `main.py` script finishes executing, but it is *not* run in a notebook unless it is explicitly called as it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2aee4-6d3d-4df8-b2e0-991c10db793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d3a0-3009-472e-b82d-3d4b5566dde1",
   "metadata": {},
   "source": [
    "## Clean up some log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3284cf0-f754-4d40-bf69-92d38cbe9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe write a script for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1af19-9196-4a41-b63d-411ebbc52bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Application logs\n",
    "! rm echo-hello.stdout\n",
    "! rm echo-hello.stderr\n",
    "\n",
    "# Remove log files if parsl-visualize is started from os.system (Option 1)\n",
    "! rm parsl_vis.stdout\n",
    "! rm parsl_vis.stderr\n",
    "\n",
    "# Remove log files if parsl-visualize is started from Parsl app (Option 2)\n",
    "! rm parsl_vis_app.stdout\n",
    "! rm parsl_vis_app.stderr\n",
    "\n",
    "# This directory contains Parsl monitoring along with other logs\n",
    "! rm -rf runinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3341c74-f45a-4851-a744-f12657c29c3b",
   "metadata": {},
   "source": [
    "## Start Parsl Monitoring - Option 3 - Post workflow manual invocation\n",
    "\n",
    "Once the Parsl `./runinfo/monitoring.db` is created, it is possible to start Parsl Monitoring and browse the results of workflow in an offline manner.  In this scenario, `parsl-visualize` can be started on the command line provided that a Conda env with `parsl[visualize]` installed is activated. For example:\n",
    "```\n",
    "source pw/.miniconda3/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "parsl-visualize sqlite:////${HOME}/mlops-parsl-workflow/runinfo/monitoring.db\n",
    "```\n",
    "(You may need to adjust the path to the Conda environment, its name, and the path to `monitoring.db`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486c478-d339-4993-9ae0-3750ea1dda49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:digits_env]",
   "language": "python",
   "name": "conda-env-digits_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
