{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a21bdaa-9df8-4503-bfc0-39e2d7efa766",
   "metadata": {},
   "source": [
    "# MLOPs Parsl workflow\n",
    "\n",
    "This notebook is the stand-alone companion to the Parsl MLOPs workflow in `main.py` in this repository. This notebook is designed to be run directly on an HPC resource while the `main.py` in this workflow uses the `parsl_utils` to launch MLOPs applications from a central coordinating node (i.e. a laptop or the Parallel Works platform). This workflow simulates a typical MLOPs situation with the following tasks:\n",
    "1. start an MLFlow tracking server\n",
    "2. start DVC tracking within an architve repository + remote\n",
    "3. download and preprocess training data\n",
    "4. run training loop and store results on-the-fly with MLFlow\n",
    "5. commit and push resulting models with DVC to repo + remote\n",
    "6. use the model for inference and generate figures.\n",
    "7. reusing the model for inference and generating figures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768cea9-9406-4d21-8d60-2d78ff2b9182",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34799ce1-6bf0-4862-a4ae-f0f089701bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda installs\n",
    "! conda install -y -c conda-forge tensorflow\n",
    "! conda install -y -c conda-forge matplotlib\n",
    "! conda install -y -c conda-forge pandas\n",
    "! conda install -y -c conda-forge dvc \n",
    "\n",
    "# pip installs\n",
    "! pip install mlflow\n",
    "! pip install 'parsl[monitoring, visualization]' # Conda does not install monitoring, so use pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b018e6-3147-4123-ab59-40146b498675",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Based on the instructions in the [Parsl Tutorial](https://parsl.readthedocs.io/en/latest/1-parsl-introduction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9576f915-2745-4aef-843e-4aa1986d6130",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 18:03:13.300422: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-12 18:03:13.318073: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-12 18:03:13.323375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-12 18:03:13.336105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.09.09\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# ml dependencies\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "\n",
    "# mlflow dependencies\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from pprint import pprint\n",
    "\n",
    "# parsl dependencies\n",
    "import parsl\n",
    "import logging\n",
    "from parsl.app.app import python_app, bash_app\n",
    "from parsl.configs.local_threads import Config\n",
    "from parsl.executors import HighThroughputExecutor # we want to use monitoring, so we must use HTEX\n",
    "from parsl.monitoring.monitoring import MonitoringHub\n",
    "from parsl.addresses import address_by_hostname\n",
    "\n",
    "#=================================================\n",
    "# Log everything to stdout (ends up in pink boxes \n",
    "# in the notebook). This information is logged anyway\n",
    "# in ./runinfo/<run_id>/parsl.log\n",
    "#parsl.set_stream_logger() # <-- log everything to stdout\n",
    "#==================================================\n",
    "\n",
    "print(parsl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6be29-0198-4394-9c9c-94b2f3e10635",
   "metadata": {},
   "source": [
    "# Configure Parsl\n",
    "\n",
    "This configuration must use the HTEX since we also want to enable [Parsl monitoring](https://parsl.readthedocs.io/en/latest/userguide/monitoring.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e57095f-292c-4b32-ae49-8382ec2fdede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "   executors=[\n",
    "       HighThroughputExecutor(\n",
    "           label=\"local_htex\",\n",
    "           cores_per_worker=1,\n",
    "           max_workers_per_node=2,\n",
    "           address=address_by_hostname(),\n",
    "       )\n",
    "   ],\n",
    "   monitoring=MonitoringHub(\n",
    "       hub_address=address_by_hostname(),\n",
    "       hub_port=55055,\n",
    "       monitoring_debug=False,\n",
    "       resource_monitoring_interval=10,\n",
    "   ),\n",
    "   strategy='none'\n",
    ")\n",
    "\n",
    "# Loading the configuration starts a Parsl DataFlowKernel\n",
    "dfk = parsl.load(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7c0f08-efee-4c9e-8191-f6373e7238e4",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 1 - direct shell invocation to background\n",
    "\n",
    "This step can be done at any point provided that a database file exists.  The default location of this file is in `./runinfo/monitoring.db` and this file is created when the Parsl configuration is loaded. When the notebook kernel is restarted, additional Parsl workflow runs' information is appended to the monitoring information in `./runinfo`. It is possible to view this information \"offline\" (i.e. no active running Parsl workflows, see Option 3, at the end of this notebook).\n",
    "\n",
    "This launch is commented out here since it is also possible to launch `parsl-visualize` from a Parsl app within the workflow, which is done below. This command is retained as a functional example. The advantage to running `parsl-visualize` as a Parsl app is that the visualization server is up and running while the workflow is running and then is shut down when the workflow is cleaned up. Otherwise, when `parsl-visualize` is launched via `os.system` the running child process can persist even after workflow shut down or notebook kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8444b703-daee-4ffa-b111-7c7dcd277c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch Parsl \n",
    "#os.system('parsl-visualize 1> parsl_vis.stdout 2> parsl_vis.stderr &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c471719b-de46-473f-8444-295ad8e51914",
   "metadata": {},
   "source": [
    "## Define Parsl apps\n",
    "\n",
    "Parsl workflows are divided into the smallest unit of execution, the app. There are two types of Parsl apps:\n",
    "1. Python apps are useful when launching pure Python code (i.e. TensorFlow)\n",
    "2. Bash apps are useful when launching tasks on the command line (i.e. starting the MLFlow server)\n",
    "\n",
    "Here, the applications are *defined* but not run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddeb06-9d92-4290-bb7f-048b1f506ddb",
   "metadata": {},
   "source": [
    "### Python Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5c288da-dbd2-49a4-aaad-8a67c472d171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the model directory for saving outputs\n",
    "env_name = \"digits_env\" # <name of your env>\n",
    "model_dir = './model-dir' \n",
    "\n",
    "@python_app\n",
    "def make_dir(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d70a816c-b89a-440e-9e8e-7fd9acae916d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def build_encoder(inputs=[], outputs=[]):\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "    \n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.seed_generator = keras.random.SeedGenerator(1337)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = ops.shape(z_mean)[0]\n",
    "            dim = ops.shape(z_mean)[1]\n",
    "            epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n",
    "            return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    latent_dim = 2\n",
    "    encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(16, activation=\"relu\")(x)\n",
    "    \n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    print(encoder.summary())\n",
    "    \n",
    "    return encoder\n",
    "\n",
    "@python_app\n",
    "def build_decoder(inputs=[], outputs=[]):\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "    \n",
    "    latent_dim = 2\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    \n",
    "    x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    print(decoder.summary())\n",
    "    \n",
    "    return decoder\n",
    "\n",
    "@python_app\n",
    "def build_model(inputs=[], outputs=[]):\n",
    "    import os\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "    # ml dependencies\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import ops\n",
    "    from keras import layers\n",
    "\n",
    "    # mlflow dependencies\n",
    "    import mlflow\n",
    "    from mlflow import MlflowClient\n",
    "    from pprint import pprint\n",
    "    \n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True)\n",
    "    \n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = ops.mean(\n",
    "                    ops.sum(\n",
    "                        keras.losses.binary_crossentropy(data, reconstruction),\n",
    "                        axis=(1, 2),\n",
    "                    )\n",
    "                )\n",
    "                kl_loss = -0.5 * (1 + z_log_var - ops.square(z_mean) - ops.exp(z_log_var))\n",
    "                kl_loss = ops.mean(ops.sum(kl_loss, axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "        \n",
    "    # model -----------------------------------------------------------------------------------------    \n",
    "    \n",
    "    vae = VAE(inputs[0], inputs[1])\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfa911e8-c432-4f08-baaf-062c9cad73f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@python_app\n",
    "def train_model(num, model, data, experiment):\n",
    "    if os.path.exists(os.path.join(model_dir, 'vae.weights.h5')): # if the model has already been trained at least once, load that model\n",
    "        model.load_weights(os.path.join(model_dir, 'vae.weights.h5'))\n",
    "    \n",
    "    mlflow.autolog()\n",
    "    \n",
    "    run_name = f\"{num}_test\" # define a run name for this iteration of training\n",
    "    artifact_path = f\"{num}\"  # define an artifact path that the model will be saved to\n",
    "    \n",
    "    # initiate the MLflow run context - training needs to happen inside of the mlflow run or you will run into problems with double logging\n",
    "    with mlflow.start_run(run_name = run_name, experiment_id = experiment) as run:\n",
    "        \n",
    "        history = model.fit(data, epochs=30, batch_size=128, callbacks = [early_stopping_cb])\n",
    "        model.save_weights(os.path.join(model_dir, 'vae.weights.h5')) # save model weights after training\n",
    "\n",
    "        hist_pd = pd.DataFrame(history.history)\n",
    "        hist_pd.to_csv(os.path.join(model_dir, f'history_{num}.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdafc570-c425-4eda-9c57-73191da7d946",
   "metadata": {},
   "source": [
    "### Bash Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4109f82b-c83f-4fcc-8718-6254ed1b5ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@bash_app\n",
    "def start_parsl_visualize(stdout='parsl_vis_app.stdout', stderr='parsl_vis_app.stderr'):\n",
    "    return 'parsl-visualize --listen 127.0.0.1 --port 8080'\n",
    "\n",
    "@bash_app\n",
    "def start_mlflow(stdout='mlflow_app.stdout', stderr='mlflow_app.stderr'):\n",
    "    return 'mlflow server --host 127.0.0.1 --port 8081'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df7db9-922a-4871-af78-2c475dfdc854",
   "metadata": {},
   "source": [
    "## Start Parsl monitoring - Option 2 - Monitoring as a Parsl app\n",
    "\n",
    "This approach is helpful if we want Parsl Monitoring processes to be cleaned up after the workflow is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ea30929-87b0-442d-a5aa-ee9cd7a7ce7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start Parsl visualization in a\n",
    "# separate cell since we only want\n",
    "# to run this app one time. This\n",
    "# invocation of parsl_visualize is\n",
    "# technically part of the workflow.\n",
    "\n",
    "parsl_future = start_parsl_visualize()\n",
    "mlflow_future = start_mlflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f20000-a71e-4702-b224-590d719504fd",
   "metadata": {},
   "source": [
    "## Run the workflow\n",
    "\n",
    "The workflow code below runs the applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24366c45-8279-4d62-80a3-d6d448e91b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726165859.620428   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.670683   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.672500   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.674765   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.676358   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.677796   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.820815   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.822806   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726165859.824310   89820 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 18:30:59.825738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1054 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n",
      "Exception in thread HTEX-Result-Queue-Thread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py\", line 718, in deserialize_keras_object\n",
      "    instance = cls.from_config(inner_config)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/models/model.py\", line 525, in from_config\n",
      "    return functional_from_config(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/models/functional.py\", line 477, in functional_from_config\n",
      "    process_layer(layer_data)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/models/functional.py\", line 461, in process_layer\n",
      "    layer = serialization_lib.deserialize_keras_object(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py\", line 694, in deserialize_keras_object\n",
      "    cls = _retrieve_class_or_fn(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py\", line 812, in _retrieve_class_or_fn\n",
      "    raise TypeError(\n",
      "TypeError: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'name': 'sampling_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}}, 'registered_name': 'Sampling', 'build_config': {'input_shape': [[None, 2], [None, 2]]}, 'name': 'sampling_2', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/parsl/process_loggers.py\", line 26, in wrapped\n",
      "    r = func(*args, **kwargs)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/parsl/executors/high_throughput/executor.py\", line 479, in _result_queue_worker\n",
      "    result = deserialize(msg['result'])\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/parsl/serialize/facade.py\", line 161, in deserialize\n",
      "    result = deserializer.deserialize(body)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/parsl/serialize/concretes.py\", line 65, in deserialize\n",
      "    return dill.loads(body)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/dill/_dill.py\", line 303, in loads\n",
      "    return load(file, ignore, **kwds)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/dill/_dill.py\", line 289, in load\n",
      "    return Unpickler(file, ignore=ignore, **kwds).load()\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/dill/_dill.py\", line 444, in load\n",
      "    obj = StockUnpickler.load(self)\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/keras_saveable.py\", line 21, in _unpickle_model\n",
      "    return saving_lib._load_model_from_fileobj(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/saving_lib.py\", line 442, in _load_model_from_fileobj\n",
      "    model = _model_from_config(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/saving_lib.py\", line 431, in _model_from_config\n",
      "    model = deserialize_keras_object(\n",
      "  File \"/home/lobielodan/pw/.miniconda3c/envs/digits_env/lib/python3.9/site-packages/keras/src/saving/serialization_lib.py\", line 720, in deserialize_keras_object\n",
      "    raise TypeError(\n",
      "TypeError: <class 'keras.src.models.functional.Functional'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n",
      "\n",
      "config={'module': 'keras.src.models.functional', 'class_name': 'Functional', 'config': {'name': 'encoder', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 28, 28, 1], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_7'}, 'registered_name': None, 'name': 'input_layer_7', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_10', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'filters': 32, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 28, 28, 1]}, 'name': 'conv2d_10', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 28, 28, 1], 'dtype': 'float32', 'keras_history': ['input_layer_7', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d_11', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}, 'filters': 64, 'kernel_size': [3, 3], 'strides': [2, 2], 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': [1, 1], 'groups': 1, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 14, 14, 32]}, 'name': 'conv2d_11', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 14, 14, 32], 'dtype': 'float32', 'keras_history': ['conv2d_10', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Flatten', 'config': {'name': 'flatten_5', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}, 'data_format': 'channels_last'}, 'registered_name': None, 'build_config': {'input_shape': [None, 7, 7, 64]}, 'name': 'flatten_5', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 7, 7, 64], 'dtype': 'float32', 'keras_history': ['conv2d_11', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_7', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}, 'units': 16, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 3136]}, 'name': 'dense_7', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 3136], 'dtype': 'float32', 'keras_history': ['flatten_5', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'z_mean', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}, 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16]}, 'name': 'z_mean', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16], 'dtype': 'float32', 'keras_history': ['dense_7', 0, 0]}}], 'kwargs': {}}]}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'z_log_var', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}, 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 16]}, 'name': 'z_log_var', 'inbound_nodes': [{'args': [{'class_name': '__keras_tensor__', 'config': {'shape': [None, 16], 'dtype': 'float32', 'keras_history': ['dense_7', 0, 0]}}], 'kwargs': {}}]}, {'module': None, 'class_name': 'Sampling', 'config': {'name': 'sampling_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}}, 'registered_name': 'Sampling', 'build_config': {'input_shape': [[None, 2], [None, 2]]}, 'name': 'sampling_2', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}], 'input_layers': [['input_layer_7', 0, 0]], 'output_layers': [['z_mean', 0, 0], ['z_log_var', 0, 0], ['sampling_2', 0, 0]]}, 'registered_name': 'Functional', 'build_config': {'input_shape': None}}.\n",
      "\n",
      "Exception encountered: Could not locate class 'Sampling'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'Sampling', 'config': {'name': 'sampling_2', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 22796530379072}}, 'registered_name': 'Sampling', 'build_config': {'input_shape': [[None, 2], [None, 2]]}, 'name': 'sampling_2', 'inbound_nodes': [{'args': [[{'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_mean', 0, 0]}}, {'class_name': '__keras_tensor__', 'config': {'shape': [None, 2], 'dtype': 'float32', 'keras_history': ['z_log_var', 0, 0]}}]], 'kwargs': {}}]}\n"
     ]
    }
   ],
   "source": [
    "# retraining the model n times\n",
    "count = 0\n",
    "n = 5\n",
    "\n",
    "(x_train, Y_train), (x_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.expand_dims(np.concatenate([x_train, x_test], axis=0), -1).astype(\"float32\") / 255\n",
    "\n",
    "encoder = build_encoder().result()\n",
    "decoder = build_decoder().result()\n",
    "model = build_model([encoder, decoder]).result()\n",
    "\n",
    "for arr in np.array_split(mnist_digits, n):\n",
    "    count += 1\n",
    "    train_model(f\"rand_{count}\", model, arr, digit_experiment3.experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742f3ea-2d2d-407f-948b-bdb583c15184",
   "metadata": {},
   "source": [
    "## Stop Parsl\n",
    "\n",
    "The cells above can be rerun any number of times; this will simply send more and more apps to be run by Parsl. When the workflow is truly complete, it is time to call the cleanup() command. This command runs implicitly when a `main.py` script finishes executing, but it is *not* run in a notebook unless it is explicitly called as it is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2aee4-6d3d-4df8-b2e0-991c10db793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b5d3a0-3009-472e-b82d-3d4b5566dde1",
   "metadata": {},
   "source": [
    "## Clean up some log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3284cf0-f754-4d40-bf69-92d38cbe9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe write a script for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd1af19-9196-4a41-b63d-411ebbc52bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Application logs\n",
    "! rm echo-hello.stdout\n",
    "! rm echo-hello.stderr\n",
    "\n",
    "# Remove log files if parsl-visualize is started from os.system (Option 1)\n",
    "! rm parsl_vis.stdout\n",
    "! rm parsl_vis.stderr\n",
    "\n",
    "# Remove log files if parsl-visualize is started from Parsl app (Option 2)\n",
    "! rm parsl_vis_app.stdout\n",
    "! rm parsl_vis_app.stderr\n",
    "\n",
    "# This directory contains Parsl monitoring along with other logs\n",
    "! rm -rf runinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3341c74-f45a-4851-a744-f12657c29c3b",
   "metadata": {},
   "source": [
    "## Start Parsl Monitoring - Option 3 - Post workflow manual invocation\n",
    "\n",
    "Once the Parsl `./runinfo/monitoring.db` is created, it is possible to start Parsl Monitoring and browse the results of workflow in an offline manner.  In this scenario, `parsl-visualize` can be started on the command line provided that a Conda env with `parsl[visualize]` installed is activated. For example:\n",
    "```\n",
    "source pw/.miniconda3/etc/profile.d/conda.sh\n",
    "conda activate base\n",
    "parsl-visualize sqlite:////${HOME}/mlops-parsl-workflow/runinfo/monitoring.db\n",
    "```\n",
    "(You may need to adjust the path to the Conda environment, its name, and the path to `monitoring.db`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f486c478-d339-4993-9ae0-3750ea1dda49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:digits_env]",
   "language": "python",
   "name": "conda-env-digits_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
